{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.sql.functions import isnan, when, count, col, udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/home/ds/notebooks/reviews_Cell_Phones_and_Accessories_5.json.gz\"\n",
    "APP_NAME = \"Amazon Reviews Random Forest\"\n",
    "SPARK_URL = \"local[*]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(APP_NAME).master(SPARK_URL).getOrCreate()\n",
    "df = spark.read.options(inferschema = \"true\").json(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+--------------------+-----------+--------------+----------------+--------------------+--------------+\n",
      "|      asin|helpful|overall|          reviewText| reviewTime|    reviewerID|    reviewerName|             summary|unixReviewTime|\n",
      "+----------+-------+-------+--------------------+-----------+--------------+----------------+--------------------+--------------+\n",
      "|120401325X| [0, 0]|    4.0|They look good an...|05 21, 2014|A30TL5EWN6DFXT|       christina|          Looks Good|    1400630400|\n",
      "|120401325X| [0, 0]|    5.0|These stickers wo...|01 14, 2014| ASY55RVNIL0UD|        emily l.|Really great prod...|    1389657600|\n",
      "|120401325X| [0, 0]|    5.0|These are awesome...|06 26, 2014|A2TMXE2AFO7ONB|           Erica|      LOVE LOVE LOVE|    1403740800|\n",
      "|120401325X| [4, 4]|    4.0|Item arrived in g...|10 21, 2013| AWJ0WZQYMYFQ4|              JM|               Cute!|    1382313600|\n",
      "|120401325X| [2, 3]|    5.0|awesome! stays on...| 02 3, 2013| ATX7CZYFXI1KW|patrice m rogoza|leopard home butt...|    1359849600|\n",
      "+----------+-------+-------+--------------------+-----------+--------------+----------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asin',\n",
       " 'helpful',\n",
       " 'overall',\n",
       " 'reviewText',\n",
       " 'reviewTime',\n",
       " 'reviewerID',\n",
       " 'reviewerName',\n",
       " 'summary',\n",
       " 'unixReviewTime']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape is 194439 rows by 9 columns.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset shape is {df.count():d} rows by {len(df.columns):d} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"helpful\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape is 194439 rows by 8 columns.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset shape is {df.count():d} rows by {len(df.columns):d} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrameNaFunctions as na\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3519 null values in the dataset.\n"
     ]
    }
   ],
   "source": [
    "null_counts = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) \n",
    "                         for c in df.columns]).toPandas().to_dict(orient='records')\n",
    "\n",
    "print(f\"There are {sum(null_counts[0].values()):d} null values in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape is 190920 rows by 8 columns.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset shape is {df.count():d} rows by {len(df.columns):d} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 null values in the dataset.\n"
     ]
    }
   ],
   "source": [
    "null_counts = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) \n",
    "                         for c in df.columns]).toPandas().to_dict(orient='records')\n",
    "\n",
    "print(f\"There are {sum(null_counts[0].values()):d} null values in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import Tokenizer, Word2Vec\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"reviewText\", outputCol=\"tokenized_text\").transform(df)\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=300, seed=42, inputCol=\"tokenized_text\", outputCol=\"w2v_vector\").fit(tokenizer)\n",
    "\n",
    "w2vdf=word2Vec.transform(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|          w2v_vector|\n",
      "+--------------------+\n",
      "|[0.04338343018615...|\n",
      "|[0.03020610212115...|\n",
      "|[0.00380904941395...|\n",
      "|[0.05073430995096...|\n",
      "|[0.01278761538560...|\n",
      "|[-0.0293635625760...|\n",
      "|[0.05099961345853...|\n",
      "|[-0.0138569975737...|\n",
      "|[0.00583547643724...|\n",
      "|[0.00207917444141...|\n",
      "|[0.04824984188945...|\n",
      "|[-0.0167498880321...|\n",
      "|[0.05679807888130...|\n",
      "|[0.00722917910534...|\n",
      "|[0.02919429005123...|\n",
      "|[-0.0121844317599...|\n",
      "|[0.02507152950958...|\n",
      "|[0.04784079593734...|\n",
      "|[0.00537914987288...|\n",
      "|[-0.0309728097968...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w2vdf.select(\"w2v_vector\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vdf = w2vdf.withColumn(\"sentiment\", when(col(\"overall\") >= 4, 1).otherwise(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|          reviewText|sentiment|\n",
      "+--------------------+---------+\n",
      "|They look good an...|        1|\n",
      "|These stickers wo...|        1|\n",
      "|These are awesome...|        1|\n",
      "|Item arrived in g...|        1|\n",
      "|awesome! stays on...|        1|\n",
      "|These make using ...|        0|\n",
      "|Came just as desc...|        1|\n",
      "|it worked for the...|        0|\n",
      "|Good case, solid ...|        1|\n",
      "|This is a fantast...|        1|\n",
      "+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w2vdf.select(\"reviewText\", \"sentiment\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|sentiment| count|\n",
      "+---------+------+\n",
      "|        1|145878|\n",
      "|        0| 45042|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w2vdf.groupBy(\"sentiment\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2359208045254557"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "45042 / (145878 + 45042)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def waste_count(text):\n",
    "    if \"waste\" in text:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_udf = udf(waste_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vdf = w2vdf.withColumn(\"waste\", waste_udf(df[\"reviewText\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cheap_count(text):\n",
    "    if \"cheap\" in text:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheap_udf = udf(cheap_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vdf = w2vdf.withColumn(\"cheap\", cheap_udf(df[\"reviewText\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def refund_count(text):\n",
    "    if \"refund\" in text:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "refund_udf = udf(refund_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vdf = w2vdf.withColumn(\"refund\", refund_udf(df[\"reviewText\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('asin', 'string'),\n",
       " ('overall', 'double'),\n",
       " ('reviewText', 'string'),\n",
       " ('reviewTime', 'string'),\n",
       " ('reviewerID', 'string'),\n",
       " ('reviewerName', 'string'),\n",
       " ('summary', 'string'),\n",
       " ('unixReviewTime', 'bigint'),\n",
       " ('tokenized_text', 'array<string>'),\n",
       " ('w2v_vector', 'vector'),\n",
       " ('sentiment', 'int'),\n",
       " ('waste', 'string'),\n",
       " ('cheap', 'string'),\n",
       " ('refund', 'string')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "w2vdf = w2vdf.withColumn(\"waste\", w2vdf[\"waste\"].cast(IntegerType()))\n",
    "w2vdf = w2vdf.withColumn(\"refund\", w2vdf[\"refund\"].cast(IntegerType()))\n",
    "w2vdf = w2vdf.withColumn(\"cheap\", w2vdf[\"cheap\"].cast(IntegerType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('asin', 'string'),\n",
       " ('overall', 'double'),\n",
       " ('reviewText', 'string'),\n",
       " ('reviewTime', 'string'),\n",
       " ('reviewerID', 'string'),\n",
       " ('reviewerName', 'string'),\n",
       " ('summary', 'string'),\n",
       " ('unixReviewTime', 'bigint'),\n",
       " ('tokenized_text', 'array<string>'),\n",
       " ('w2v_vector', 'vector'),\n",
       " ('sentiment', 'int'),\n",
       " ('waste', 'int'),\n",
       " ('cheap', 'int'),\n",
       " ('refund', 'int')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vdf = VectorAssembler(inputCols=[\"refund\", \"cheap\", \"waste\", \"w2v_vector\"], outputCol=\"features\").transform(w2vdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA_RATIO = .8\n",
    "RF_NUM_TREES = 20\n",
    "RF_MAX_DEPTH = 10\n",
    "RF_NUM_BINS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"indexedLabel\").fit(w2vdf)\n",
    "\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(w2vdf)\n",
    "\n",
    "(trainingData, testData) = w2vdf.randomSplit([TRAINING_DATA_RATIO, 1 - TRAINING_DATA_RATIO])\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=RF_NUM_TREES)\n",
    "\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.236691\n",
      "Accuracy = 0.763309\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Test Error = {(1.0 - accuracy):g}\")\n",
    "print(f\"Accuracy = {accuracy:g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+---+\n",
      "|sentiment_prediction|  0.0|1.0|\n",
      "+--------------------+-----+---+\n",
      "|                   1|29257| 10|\n",
      "|                   0| 9091| 93|\n",
      "+--------------------+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "predictions.crosstab(\"sentiment\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
